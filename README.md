# Multi-modality Evals
**Multi-modality Evals** is an evaulation and benchmarking framework for multi-modality AI/ML models on a large number of different evaluation tasks. Evaluates AI/ML models on metrics such as answer relevancy, summarization, faithfulness, hallucination, toxicity, bias, etc.

<div align="center">

</div>

### Overview

#### Features

Benchmark LLMs on popular LLM benchmarks datasets.

### Directory Structure

The directory structure for this repo is as follows,

```
├── docs # MD used to generate readthedocs
├── multi-modality-evals # Core evaluation and benchmarking scripts
│ ├── benchmarks
│ ├── metrics
│ ├── models
│ ├── tasks
│ └── prompts
├── scripts # Python utility scripts
├── examples # Evaluation and benchmarking scripts
└── tests # Test suite run at each PR
```

### Install

### Usage

#### Datasets

#### Models


#### Community

We welcome contributions from the AI community! Read this [guide]() to get started.

Leave us a star, it helps the project to get discovered by others and keeps us motivated to build awesome open-source tools!
